{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys \n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import hydra\n",
    "import lightning as L\n",
    "import torch\n",
    "from model import (\n",
    "    BhvrDecoder,\n",
    "    ContextManager,\n",
    "    Decoder,\n",
    "    Encoder,\n",
    "    MaeMaskManager,\n",
    "    SpikesPatchifier,\n",
    "    SslDecoder,\n",
    "    NDT2Model,\n",
    "    # MTMMaskManager\n",
    ")\n",
    "from transforms import FilterUnit, Ndt2Tokenizer\n",
    "from train import DataModule\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from torch import optim\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "from train import TrainWrapper, set_callbacks\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# probe_superv\n",
    "# def load_cfg():\n",
    "#     sys.argv = [sys.argv[0]]\n",
    "#     cfg = OmegaConf.load(\"./configs/probe_superv.yaml\")\n",
    "#     dflt_cfg = OmegaConf.load(\"./configs/_default.yaml\")\n",
    "#     cfg = OmegaConf.merge(cfg, dflt_cfg)\n",
    "#     cfg.dataset = OmegaConf.load(\"./configs/dataset/test.yaml\")\n",
    "#     del cfg.defaults\n",
    "#     return cfg\n",
    "# cfg.dataset[0].selection[0].sessions = [cfg.dataset[0].selection[0].sessions[0]]\n",
    "\n",
    "def load_cfg():\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    cfg = OmegaConf.load(\"./configs/train_ssl.yaml\")\n",
    "    dflt_cfg = OmegaConf.load(\"./configs/_default.yaml\")\n",
    "    cfg = OmegaConf.merge(cfg, dflt_cfg)\n",
    "    cfg.dataset = OmegaConf.load(\"./configs/dataset/train.yaml\")\n",
    "    del cfg.defaults\n",
    "    return cfg\n",
    "\n",
    "cfg = load_cfg()\n",
    "cfg.wandb.enable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all entries except for brainset inside cfg['dataset'][0].selection[0]\n",
    "cfg['dataset'][0].selection[0] = {'brainset': 'perich_miller_population_2018'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512, 'mask_ratio': 0.5, 'is_ssl': True, 'unsorted': True, 'keep_M1_units': True, 'train_ratio': 0.8, 'optimizer': {'scheduler': True, 'lr': 0.0005, 'weight_decay': 0.01, 'start_factor': 0.1, 'warmup_steps': 100, 'decay_steps': 2500, 'lr_min': 1e-06}, 'load_from_checkpoint': False, 'callbacks': {'checkpoint': True, 'early_stop': True, 'patience': 250}, 'wandb': {'enable': False, 'entity': 'aandre8-gatech', 'project': 'reproduce_NDT2', 'run_name': None}, 'seed': 0, 'split_seed': 0, 'superv_batch_size': 512, 'epochs': 800, 'eval_epochs': 1, 'precision': 'bf16-mixed', 'num_workers': 16, 'log_dir': './logs', 'log_every_n_steps': 1, 'fast_dev_run': False, 'num_sanity_val_steps': 0, 'model': {'dim': 256, 'max_time_patches': 256, 'max_space_patches': 256, 'patchifier': {}, 'encoder': {'depth': 6, 'heads': 4, 'dropout': 0.1, 'ffn_mult': 1}, 'predictor': {'depth': 2, 'heads': 4, 'dropout': 0.1, 'ffn_mult': 1}, 'bhv_decoder': {'depth': 2, 'heads': 4, 'dropout': 0.1, 'ffn_mult': 1, 'decode_time_pool': 'mean', 'behavior_dim': 2, 'behavior_lag': 0.12}}, 'subtask_idx': 2, 'data_root': './data/process', 'ctx_time': 1.0, 'bin_time': 0.02, 'patch_size': [32, 1], 'pad_val': 64, 'dataset': [{'selection': [{'brainset': 'perich_miller_population_2018'}]}], 'batch_size_per_gpu': 512, 'superv_batch_size_per_gpu': 512}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(cfg.seed)\n",
    "\n",
    "if cfg.fast_dev_run:\n",
    "    cfg.wandb.enable = False\n",
    "    cfg.num_workers = 0\n",
    "\n",
    "\n",
    "with open_dict(cfg):\n",
    "    # Adjust batch size for multi-gpu\n",
    "    num_gpus = torch.cuda.device_count() + 1\n",
    "    cfg.batch_size_per_gpu = cfg.batch_size // num_gpus\n",
    "    cfg.superv_batch_size = cfg.superv_batch_size or cfg.batch_size\n",
    "    cfg.superv_batch_size_per_gpu = cfg.superv_batch_size // num_gpus\n",
    "    log.info(f\"Number of GPUs: {num_gpus}\")\n",
    "    log.info(f\"Batch size per GPU: {cfg.batch_size_per_gpu}\")\n",
    "    log.info(f\"Superv batch size per GPU: {cfg.superv_batch_size_per_gpu}\")\n",
    "\n",
    "dim = cfg.model.dim\n",
    "\n",
    "# Mask manager (for MAE SSL)\n",
    "mae_mask_manager = None\n",
    "if cfg.is_ssl:\n",
    "    mae_mask_manager = MaeMaskManager(cfg.mask_ratio)\n",
    "    # mae_mask_manager = MTMMaskManager(cfg.mask_ratio)\n",
    "\n",
    "# context manager\n",
    "ctx_manager = ContextManager(dim)\n",
    "\n",
    "# Spikes patchifier\n",
    "spikes_patchifier = SpikesPatchifier(dim, cfg.patch_size)\n",
    "\n",
    "# Model = Encoder + Decoder\n",
    "encoder = Encoder(\n",
    "    dim=dim,\n",
    "    max_time_patches=cfg.model.max_time_patches,\n",
    "    max_space_patches=cfg.model.max_space_patches,\n",
    "    **cfg.model.encoder,\n",
    ")\n",
    "\n",
    "if cfg.is_ssl:\n",
    "    decoder = SslDecoder(\n",
    "        dim=dim,\n",
    "        max_time_patches=cfg.model.max_time_patches,\n",
    "        max_space_patches=cfg.model.max_space_patches,\n",
    "        patch_size=cfg.patch_size,\n",
    "        **cfg.model.predictor,\n",
    "    )\n",
    "else:\n",
    "    decoder = BhvrDecoder(\n",
    "        dim=dim,\n",
    "        max_time_patches=cfg.model.max_time_patches,\n",
    "        max_space_patches=cfg.model.max_space_patches,\n",
    "        bin_time=cfg.bin_time,\n",
    "        **cfg.model.bhv_decoder,\n",
    "    )\n",
    "\n",
    "model = NDT2Model(\n",
    "        mae_mask_manager, ctx_manager, spikes_patchifier, encoder, decoder\n",
    "    )\n",
    "\n",
    "# Train wrapper\n",
    "train_wrapper = TrainWrapper(\n",
    "    cfg, model\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "ctx_tokenizer = ctx_manager.get_ctx_tokenizer()\n",
    "tokenizer = Ndt2Tokenizer(\n",
    "    ctx_time=cfg.ctx_time,\n",
    "    bin_time=cfg.bin_time,\n",
    "    patch_size=cfg.patch_size,\n",
    "    pad_val=cfg.pad_val,\n",
    "    ctx_tokenizer=ctx_tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data leakage check is disabled. Please be absolutely sure that there is no leakage between None and other splits.\n",
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# set up data module\n",
    "data_module = DataModule(cfg, tokenizer, cfg.is_ssl)\n",
    "data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# register context\n",
    "ctx_manager.init_vocab(data_module.get_ctx_vocab(ctx_manager.keys))\n",
    "\n",
    "L.seed_everything(cfg.seed)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = set_callbacks(cfg)\n",
    "\n",
    "# Set up trainer\n",
    "# trainer = L.Trainer(\n",
    "#     logger=wandb_logger,\n",
    "#     default_root_dir=cfg.log_dir,\n",
    "#     check_val_every_n_epoch=cfg.eval_epochs,\n",
    "#     max_epochs=cfg.epochs,\n",
    "#     log_every_n_steps=cfg.log_every_n_steps,\n",
    "#     callbacks=callbacks,\n",
    "#     accelerator=\"gpu\",\n",
    "#     precision=cfg.precision,\n",
    "#     fast_dev_run=cfg.fast_dev_run,\n",
    "#     num_sanity_val_steps=cfg.num_sanity_val_steps,\n",
    "#     strategy=\"ddp_find_unused_parameters_true\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.ctx_manager.session_emb.weight',\n",
       " 'model.ctx_manager.subject_emb.weight',\n",
       " 'model.decoder.cls_token',\n",
       " 'model.decoder.decoder.transformer.layers.0.self_attn.in_proj_weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.self_attn.in_proj_bias',\n",
       " 'model.decoder.decoder.transformer.layers.0.self_attn.out_proj.weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.self_attn.out_proj.bias',\n",
       " 'model.decoder.decoder.transformer.layers.0.linear1.weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.linear1.bias',\n",
       " 'model.decoder.decoder.transformer.layers.0.linear2.weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.linear2.bias',\n",
       " 'model.decoder.decoder.transformer.layers.0.norm1.weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.norm1.bias',\n",
       " 'model.decoder.decoder.transformer.layers.0.norm2.weight',\n",
       " 'model.decoder.decoder.transformer.layers.0.norm2.bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.self_attn.in_proj_weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.self_attn.in_proj_bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.self_attn.out_proj.weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.self_attn.out_proj.bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.linear1.weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.linear1.bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.linear2.weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.linear2.bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.norm1.weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.norm1.bias',\n",
       " 'model.decoder.decoder.transformer.layers.1.norm2.weight',\n",
       " 'model.decoder.decoder.transformer.layers.1.norm2.bias',\n",
       " 'model.decoder.decoder.positional_encoding.time_emb.weight',\n",
       " 'model.decoder.decoder.positional_encoding.space_emb.weight',\n",
       " 'model.decoder.out.weight',\n",
       " 'model.decoder.out.bias']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    \"model.ctx_manager.session_emb.weight\",\n",
    "    \"model.ctx_manager.subject_emb.weight\",\n",
    "    \"model.decoder.cls_token\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.self_attn.in_proj_weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.self_attn.in_proj_bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.self_attn.out_proj.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.self_attn.out_proj.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.linear1.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.linear1.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.linear2.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.linear2.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.norm1.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.norm1.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.norm2.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.0.norm2.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.self_attn.in_proj_weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.self_attn.in_proj_bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.self_attn.out_proj.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.self_attn.out_proj.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.linear1.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.linear1.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.linear2.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.linear2.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.norm1.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.norm1.bias\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.norm2.weight\",\n",
    "    \"model.decoder.decoder.transformer.layers.1.norm2.bias\",\n",
    "    \"model.decoder.decoder.positional_encoding.time_emb.weight\",\n",
    "    \"model.decoder.decoder.positional_encoding.space_emb.weight\",\n",
    "    \"model.decoder.out.weight\",\n",
    "    \"model.decoder.out.bias\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
